{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation_Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPa9EXRYCH9YivbtE4IW6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vigneswaran978/Deep-Learning-with-python/blob/master/Translation_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9tQoFWhGdJM"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yPLZGOiGjL2"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
        "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qn7zP3wIeFk"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdwkkKPLGjIc"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "  import keras.backend as K\n",
        "  if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
        "    from keras.layers import CuDNNLSTM as LSTM\n",
        "    from keras.layers import CuDNNGRU as GRU\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl7IJa75GjFP"
      },
      "source": [
        "# make sure we do softmax over the time axis\n",
        "# expected shape is N x T x D\n",
        "# note: the latest version of Keras allows you to pass in axis arg\n",
        "def softmax_over_time(x):\n",
        "  assert(K.ndim(x) > 2)\n",
        "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
        "  s = K.sum(e, axis=1, keepdims=True)\n",
        "  return e / s\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeTVuz2RGjB7"
      },
      "source": [
        "# config\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LATENT_DIM = 400\n",
        "LATENT_DIM_DECODER = 400 # idea: make it different to ensure things all fit together properly!\n",
        "NUM_SAMPLES = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JusSKUdHGi-1"
      },
      "source": [
        "# Where we will store the data\n",
        "input_texts = [] # sentence in original language\n",
        "target_texts = [] # sentence in target language\n",
        "target_texts_inputs = [] # sentence in target language offset by 1\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3NbSOQCGi79",
        "outputId": "6f6ec43f-bd63-4512-c3ed-9ec8e456f62d"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE46L3gvGi4c",
        "outputId": "02df9306-c439-4522-81c8-f0ef290a274c"
      },
      "source": [
        "t = 0\n",
        "for line in open('/content/drive/MyDrive/translation_model_data/fra-eng/fra.txt'):\n",
        "  # only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "\n",
        "  # input and target are separated by tab\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input and translation\n",
        "  input_text, translation, *rest = line.rstrip().split('\\t')\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + ' <eos>'\n",
        "  target_text_input = '<sos> ' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "print(\"num samples:\", len(input_texts))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples: 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GAS5Fb_RGi1c",
        "outputId": "2947015d-187f-4a3f-809e-fca32f00c2d3"
      },
      "source": [
        "#input_texts"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'They all entered.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XZ4085_Giyd"
      },
      "source": [
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI0C2qBFGivf",
        "outputId": "9b60ace0-4c38-4f3d-e385-4d328b44a114"
      },
      "source": [
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3439 unique input tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f6y7UV7Gisn"
      },
      "source": [
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHCNfLKDGipv"
      },
      "source": [
        "# tokenize the outputs\n",
        "# don't filter out special characters\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrXEzKJwGinG",
        "outputId": "ef3b7465-7832-4aea-f285-5ab2cf2a65c6"
      },
      "source": [
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9495 unique output tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5YcV4Y5Gikc"
      },
      "source": [
        "# store number of output words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmo_5qaPGih5",
        "outputId": "d2dd9038-a591-42f8-e87b-1e94ad367aba"
      },
      "source": [
        "# pad the sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_data.shape: (20000, 5)\n",
            "encoder_data[0]: [ 0  0  0  0 19]\n",
            "decoder_data[0]: [ 2 51  4  0  0  0  0  0  0  0  0  0]\n",
            "decoder_data.shape: (20000, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnRxzGuwLSZu",
        "outputId": "a6ded8cc-9ee6-484d-d317-b798a0ddce4f"
      },
      "source": [
        "#/content/drive/MyDrive/GLOVE_Embeddings/glove/glove.6B.100d.txt/glove.6B.100d.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/GLOVE_Embeddings/glove/glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM2S4czbGifQ",
        "outputId": "97df0e6b-64b1-4850-9118-19d972126fbe"
      },
      "source": [
        "\n",
        "# store all the pre-trained word vectors\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with open(os.path.join('/content/drive/MyDrive/GLOVE_Embeddings/glove/glove.6B.100d.txt/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH4ion4sGich",
        "outputId": "225950b0-9b42-456e-8154-ea28b9d90459"
      },
      "source": [
        "\n",
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvpdPzXQGiZv"
      },
      "source": [
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len_input,\n",
        "  # trainable=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6rnX_PCGiW_"
      },
      "source": [
        "# create targets, since we cannot use sparse\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "  (\n",
        "    len(input_texts),\n",
        "    max_len_target,\n",
        "    num_words_output\n",
        "  ),\n",
        "  dtype='float32'\n",
        ")\n",
        "\n",
        "# assign the values\n",
        "for i, d in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(d):\n",
        "    if word > 0:\n",
        "      decoder_targets_one_hot[i, t, word] = 1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8AG4a-QGiUV"
      },
      "source": [
        "##### build the model #####\n",
        "\n",
        "# Set up the encoder - simple!\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = Bidirectional(LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_sequences=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        "))\n",
        "encoder_outputs = encoder(x)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDl9fuAQGiRz"
      },
      "source": [
        "# Set up the decoder - not so simple\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtHczTIZGiPX"
      },
      "source": [
        "######### Attention #########\n",
        "# Attention layers need to be global because\n",
        "# they will be repeated Ty times at the decoder\n",
        "attn_repeat_layer = RepeatVector(max_len_input)\n",
        "attn_concat_layer = Concatenate(axis=-1)\n",
        "attn_dense1 = Dense(10, activation='tanh')\n",
        "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
        "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ6yfBcpGiM1"
      },
      "source": [
        "def one_step_attention(h, st_1):\n",
        "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
        "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
        " \n",
        "  # copy s(t-1) Tx times\n",
        "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
        "  st_1 = attn_repeat_layer(st_1)\n",
        "\n",
        "  # Concatenate all h(t)'s with s(t-1)\n",
        "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
        "  x = attn_concat_layer([h, st_1])\n",
        "\n",
        "  # Neural net first layer\n",
        "  x = attn_dense1(x)\n",
        "\n",
        "  # Neural net second layer with special softmax over time\n",
        "  alphas = attn_dense2(x)\n",
        "\n",
        "  # \"Dot\" the alphas and the h's\n",
        "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
        "  context = attn_dot([alphas, h])\n",
        "\n",
        "  return context\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMpYVl79GiKT"
      },
      "source": [
        "\n",
        "# define the rest of the decoder (after attention)\n",
        "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "\n",
        "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
        "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
        "context_last_word_concat_layer = Concatenate(axis=2)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "get_uyO5GiHx"
      },
      "source": [
        "# Unlike previous seq2seq, we cannot get the output\n",
        "# all in one step\n",
        "# Instead we need to do Ty steps\n",
        "# And in each of those steps, we need to consider\n",
        "# all Tx h's\n",
        "\n",
        "# s, c will be re-assigned in each iteration of the loop\n",
        "s = initial_s\n",
        "c = initial_c"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVGmDRzuGiFW"
      },
      "source": [
        "# collect outputs in a list at first\n",
        "outputs = []\n",
        "for t in range(max_len_target): # Ty times\n",
        "  # get the context using attention\n",
        "  context = one_step_attention(encoder_outputs, s)\n",
        "\n",
        "  # we need a different layer for each time step\n",
        "  selector = Lambda(lambda x: x[:, t:t+1])\n",
        "  xt = selector(decoder_inputs_x)\n",
        "  \n",
        "  # combine \n",
        "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
        "\n",
        "  # pass the combined [context, last word] into the LSTM\n",
        "  # along with [s, c]\n",
        "  # get the new [s, c] and output\n",
        "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
        "\n",
        "  # final dense layer to get next word prediction\n",
        "  decoder_outputs = decoder_dense(o)\n",
        "  outputs.append(decoder_outputs)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieTQxIKaGiCy"
      },
      "source": [
        "# 'outputs' is now a list of length Ty\n",
        "# each element is of shape (batch size, output vocab size)\n",
        "# therefore if we simply stack all the outputs into 1 tensor\n",
        "# it would be of shape T x N x D\n",
        "# we would like it to be of shape N x T x D\n",
        "\n",
        "def stack_and_transpose(x):\n",
        "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
        "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
        "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
        "  return x\n",
        "\n",
        "# make it a layer\n",
        "stacker = Lambda(stack_and_transpose)\n",
        "outputs = stacker(outputs)\n",
        "\n",
        "# create the model\n",
        "model = Model(\n",
        "  inputs=[\n",
        "    encoder_inputs_placeholder,\n",
        "    decoder_inputs_placeholder,\n",
        "    initial_s, \n",
        "    initial_c,\n",
        "  ],\n",
        "  outputs=outputs\n",
        ")\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgoIZfXMGiAJ"
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "  # both are of shape N x T x K\n",
        "  mask = K.cast(y_true > 0, dtype='float32')\n",
        "  out = mask * y_true * K.log(y_pred)\n",
        "  return -K.sum(out) / K.sum(mask)\n",
        "\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "  # both are of shape N x T x K\n",
        "  targ = K.argmax(y_true, axis=-1)\n",
        "  pred = K.argmax(y_pred, axis=-1)\n",
        "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
        "\n",
        "  # 0 is padding, don't include those\n",
        "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
        "  n_correct = K.sum(mask * correct)\n",
        "  n_total = K.sum(mask)\n",
        "  return n_correct / n_total\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYMd0LG-M9rw",
        "outputId": "9ab8df29-5f9e-4165-9347-3222f23bab95"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# train the model\n",
        "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
        "r = model.fit(\n",
        "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2\n",
        ")\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "250/250 [==============================] - 297s 1s/step - loss: 6.4344 - acc: 0.2294 - val_loss: 6.0209 - val_acc: 0.2351\n",
            "Epoch 2/30\n",
            "250/250 [==============================] - 245s 981ms/step - loss: 5.2954 - acc: 0.2672 - val_loss: 5.7660 - val_acc: 0.2436\n",
            "Epoch 3/30\n",
            "250/250 [==============================] - 244s 978ms/step - loss: 4.9372 - acc: 0.2839 - val_loss: 5.4624 - val_acc: 0.2657\n",
            "Epoch 4/30\n",
            "250/250 [==============================] - 246s 984ms/step - loss: 4.4457 - acc: 0.3382 - val_loss: 5.1121 - val_acc: 0.3147\n",
            "Epoch 5/30\n",
            "250/250 [==============================] - 247s 990ms/step - loss: 3.9417 - acc: 0.3867 - val_loss: 4.8114 - val_acc: 0.3498\n",
            "Epoch 6/30\n",
            "250/250 [==============================] - 246s 984ms/step - loss: 3.5016 - acc: 0.4285 - val_loss: 4.6700 - val_acc: 0.3720\n",
            "Epoch 7/30\n",
            "250/250 [==============================] - 245s 981ms/step - loss: 3.1157 - acc: 0.4550 - val_loss: 4.5720 - val_acc: 0.3867\n",
            "Epoch 8/30\n",
            "250/250 [==============================] - 246s 984ms/step - loss: 2.7806 - acc: 0.4761 - val_loss: 4.5102 - val_acc: 0.3933\n",
            "Epoch 9/30\n",
            "250/250 [==============================] - 246s 985ms/step - loss: 2.4657 - acc: 0.5015 - val_loss: 4.4660 - val_acc: 0.4058\n",
            "Epoch 10/30\n",
            "250/250 [==============================] - 247s 987ms/step - loss: 2.2025 - acc: 0.5226 - val_loss: 4.4261 - val_acc: 0.4028\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - 246s 985ms/step - loss: 1.9594 - acc: 0.5506 - val_loss: 4.4389 - val_acc: 0.4088\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - 244s 976ms/step - loss: 1.7498 - acc: 0.5767 - val_loss: 4.4363 - val_acc: 0.4146\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 247s 989ms/step - loss: 1.5697 - acc: 0.6042 - val_loss: 4.4314 - val_acc: 0.4177\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - 248s 992ms/step - loss: 1.4048 - acc: 0.6303 - val_loss: 4.4311 - val_acc: 0.4188\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - 247s 987ms/step - loss: 1.2801 - acc: 0.6527 - val_loss: 4.4546 - val_acc: 0.4197\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 245s 980ms/step - loss: 1.1652 - acc: 0.6710 - val_loss: 4.4871 - val_acc: 0.4227\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 245s 978ms/step - loss: 1.0620 - acc: 0.6886 - val_loss: 4.5065 - val_acc: 0.4232\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 245s 980ms/step - loss: 0.9902 - acc: 0.7027 - val_loss: 4.5446 - val_acc: 0.4285\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - 249s 997ms/step - loss: 0.9275 - acc: 0.7105 - val_loss: 4.5815 - val_acc: 0.4295\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 250s 1000ms/step - loss: 0.8596 - acc: 0.7234 - val_loss: 4.6171 - val_acc: 0.4268\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 250s 1000ms/step - loss: 0.8056 - acc: 0.7351 - val_loss: 4.6499 - val_acc: 0.4301\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 249s 995ms/step - loss: 0.7747 - acc: 0.7369 - val_loss: 4.6683 - val_acc: 0.4329\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - 247s 986ms/step - loss: 0.7324 - acc: 0.7462 - val_loss: 4.7056 - val_acc: 0.4284\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 246s 983ms/step - loss: 0.7066 - acc: 0.7498 - val_loss: 4.7430 - val_acc: 0.4304\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 244s 978ms/step - loss: 0.6785 - acc: 0.7533 - val_loss: 4.7688 - val_acc: 0.4321\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 245s 980ms/step - loss: 0.6408 - acc: 0.7635 - val_loss: 4.7864 - val_acc: 0.4296\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 245s 979ms/step - loss: 0.6239 - acc: 0.7614 - val_loss: 4.8201 - val_acc: 0.4300\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 244s 978ms/step - loss: 0.6051 - acc: 0.7644 - val_loss: 4.8686 - val_acc: 0.4341\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 245s 980ms/step - loss: 0.5849 - acc: 0.7708 - val_loss: 4.8962 - val_acc: 0.4318\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 245s 978ms/step - loss: 0.5601 - acc: 0.7761 - val_loss: 4.9727 - val_acc: 0.4320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "Oa5Y7zPoM9oN",
        "outputId": "163750ea-049f-4123-97ce-d9f483775a69"
      },
      "source": [
        "# plot some data\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-878983cf1e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici1goVpM9lc"
      },
      "source": [
        "\n",
        "##### Make predictions #####\n",
        "# As with the poetry example, we need to create another model\n",
        "# that can take in the RNN state and previous word as input\n",
        "# and accept a T=1 sequence.\n",
        "\n",
        "# The encoder will be stand-alone\n",
        "# From this we will get our initial decoder hidden state\n",
        "# i.e. h(1), ..., h(Tx)\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
        "\n",
        "# next we define a T=1 decoder model\n",
        "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "\n",
        "# no need to loop over attention steps this time because there is only one step\n",
        "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
        "\n",
        "# combine context with last word\n",
        "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTateL0jM9iy"
      },
      "source": [
        "# lstm and final dense\n",
        "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
        "decoder_outputs = decoder_dense(o)\n",
        "\n",
        "\n",
        "# note: we don't really need the final stack and tranpose\n",
        "# because there's only 1 output\n",
        "# it is already of size N x D\n",
        "# no need to make it 1 x N x D --> N x 1 x D\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bV9jdclM9gY"
      },
      "source": [
        "# create the model object\n",
        "decoder_model = Model(\n",
        "  inputs=[\n",
        "    decoder_inputs_single,\n",
        "    encoder_outputs_as_input,\n",
        "    initial_s, \n",
        "    initial_c\n",
        "  ],\n",
        "  outputs=[decoder_outputs, s, c]\n",
        ")\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1EKbyZkRxdG"
      },
      "source": [
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZTVxACWRxZk"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  enc_out = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "\n",
        "  # [s, c] will be updated in each loop iteration\n",
        "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
        "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
        "\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
        "        \n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(o.flatten())\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "  return ' '.join(output_sentence)\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5caFlPORxW6",
        "outputId": "160c75a4-d82d-43da-cfda-e042e81b9583"
      },
      "source": [
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(input_texts))\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_texts[i])\n",
        "  print('Predicted translation:', translation)\n",
        "  print('Actual translation:', target_texts[i])\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: I grow tomatoes.\n",
            "Predicted translation: je cultive des tomates.\n",
            "Actual translation: Je cultive des tomates. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Where's my room?\n",
            "Predicted translation: où est ma voiture ? ?\n",
            "Actual translation: Où est ma chambre ? <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Is she at home?\n",
            "Predicted translation: est-elle à la ?\n",
            "Actual translation: Est-elle chez elle ? <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: This is cheap.\n",
            "Predicted translation: c'est bon marché.\n",
            "Actual translation: C'est bon marché. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Did Tom vote?\n",
            "Predicted translation: tom a voté ?\n",
            "Actual translation: Tom a voté ? <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: It's unlocked.\n",
            "Predicted translation: c'est déverrouillé.\n",
            "Actual translation: C'est déverrouillé. <eos>\n",
            "Continue? [Y/n]N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwL_yFTIRxUZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evX_txwoRxR7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFXtup5RxPT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3WO0XSIRxNQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5pVGv70RxKG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmpWxg_rRxHx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCwqQiDnRxFD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3sYUVDsRxCS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiilKZoRw_q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glLjB-W2Rw8_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2I-kpCnRw6i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-LmzwWLRw3-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QmdC_RURw1e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vGjxkzLGh9g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}